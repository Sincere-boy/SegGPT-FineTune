import os
import json
import time
import numpy as np
import copy
from typing import List, Tuple, Dict, Any, Optional

import torch
import torch.nn.functional as F
from torch import nn
from PIL import Image
from transformers import SegGptImageProcessor, SegGptForImageSegmentation

# ======================
# å¯é…å‚æ•°
# ======================
INPUT_DIR   = "images/test_dir"  # ç”¨äºç»Ÿè®¡çš„å›¾ç‰‡ç›®å½•
OUTPUT_DIR  = "result/test_dir"  # ç»Ÿè®¡ç»“æœä¿å­˜ç›®å½•
PROMPT_DIR  = "prompt_images/ball"  # æç¤ºå›¾ä¸æ©ç æ‰€åœ¨ç›®å½•
CHECKPOINT  = "checkpoint/seggpt-vit-large"

DEVICE      = torch.device("cuda" if torch.cuda.is_available() else "cpu")
FEATURE_ENSEMBLE = True                   # ä¸çº¿ä¸Šä¸€è‡´æ—¶ç»Ÿè®¡æ›´å¯ä¿¡
MAX_CALIB   = 50                          # æœ€å¤šç»Ÿè®¡å¤šå°‘å¼ å›¾ç‰‡ï¼ˆå¯è°ƒï¼Œå¤§ä¸€ç‚¹æ›´ç¨³ï¼‰

# ======================
# é€šç”¨å·¥å…·
# ======================
def log(msg: str):
    print(f"[SegGPT-Stats] {msg}")

def list_images(folder: str) -> List[str]:
    exts = (".jpeg", ".jpg", ".png", ".bmp", ".webp", ".tif", ".tiff")
    return [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(exts)]

def to_bs_c(x: torch.Tensor) -> torch.Tensor:
    """
    æŠŠéšè—æ€ç»Ÿä¸€æˆ (B*S, C) ä»¥ä¾¿è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
    æ”¯æŒ (B, Hp, Wp, C) æˆ– (B, S, C) ä¸¤ç§ã€‚
    """
    if x.dim() == 4:  # (B, Hp, Wp, C)
        B, H, W, C = x.shape
        x = x.reshape(B, H * W, C)
    elif x.dim() == 3:  # (B, S, C)
        pass
    else:
        raise ValueError(f"Unexpected hidden state shape: {x.shape}")
    B, S, C = x.shape
    return x.reshape(B * S, C)

def nhwc_adapt_to(x: torch.Tensor, H: int, W: int) -> torch.Tensor:
    """
    æŠŠ NHWC çš„å¼ é‡ï¼Œé€šè¿‡è‡ªé€‚åº”æ± åŒ–è°ƒæ•´åˆ°ç›®æ ‡ HÃ—Wï¼ˆé€šé“ä¸æ‰¹å¤§å°ä¸å˜ï¼‰ã€‚
    """
    assert x.dim() == 4
    x = x.permute(0, 3, 1, 2).contiguous()  # NHWC -> NCHW
    x = F.adaptive_avg_pool2d(x, output_size=(H, W))
    x = x.permute(0, 2, 3, 1).contiguous()  # NCHW -> NHWC
    return x

def align_for_similarity(a: torch.Tensor, b: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    å¯¹é½åŒä¸€ block çš„è¾“å…¥ aã€è¾“å‡º b çš„å½¢çŠ¶ã€‚
    æ­£å¸¸æƒ…å†µä¸‹ä¸¤è€…å½¢çŠ¶ä¸€è‡´ï¼›è‹¥ä¸ä¸€è‡´ï¼Œåšç¨³å¥å¯¹é½ä»¥é¿å…æŠ¥é”™ï¼š
      - 4D NHWCï¼šå¯¹ b åšè‡ªé€‚åº”æ± åŒ–åˆ° a çš„ (H, W)ï¼›
      - 3D (B, S, C)ï¼šæˆªæ–­åˆ°ç›¸åŒçš„æœ€å° Sã€‚
    """
    if a.dim() == 4 and b.dim() == 4:
        Ba, Ha, Wa, Ca = a.shape
        Bb, Hb, Wb, Cb = b.shape
        assert Ba == Bb and Ca == Cb, f"Batch/Channel mismatch: {a.shape} vs {b.shape}"
        if (Ha, Wa) != (Hb, Wb):
            b = nhwc_adapt_to(b, Ha, Wa)
        return a, b

    if a.dim() == 3 and b.dim() == 3:
        Ba, Sa, Ca = a.shape
        Bb, Sb, Cb = b.shape
        assert Ba == Bb and Ca == Cb, f"Batch/Channel mismatch: {a.shape} vs {b.shape}"
        if Sa != Sb:
            S = min(Sa, Sb)
            a = a[:, :S, :]
            b = b[:, :S, :]
        return a, b

    # å…œåº•ï¼šä¸åŒç»´åº¦æ—¶ï¼Œå…¨éƒ¨å±•å¹³æˆ (B*S, C) åå†å¯¹é½åˆ°æœ€å°é•¿åº¦
    A = to_bs_c(a)
    B = to_bs_c(b)
    S = min(A.size(0), B.size(0))
    return A[:S], B[:S]

def cosine_mean(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """
    è®¡ç®—è¾“å…¥/è¾“å‡ºçš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆé€ token åæ±‚å‡å€¼ï¼‰ã€‚
    å†…éƒ¨åšå½’ä¸€åŒ–ï¼Œæ•°å€¼ç¨³å¥ã€‚
    """
    a, b = align_for_similarity(a, b)
    A = to_bs_c(a)
    B = to_bs_c(b)
    A = F.normalize(A, dim=-1, eps=1e-6)
    B = F.normalize(B, dim=-1, eps=1e-6)
    return (A * B).sum(dim=-1).mean()

# ======================
# Hook é‡‡é›†å™¨ï¼ˆä¿®å¤ï¼šç¨³å¥æå– Tensorï¼‰
# ======================
def extract_tensor(obj: Any) -> Optional[torch.Tensor]:
    """
    ä»å¸¸è§çš„è¾“å‡ºç»“æ„ä¸­ç¨³å¥åœ°æå–ä»£è¡¨ hidden state çš„ Tensorï¼š
    - Tensor: ç›´æ¥è¿”å›
    - tuple/list: è¿”å›ç¬¬ä¸€ä¸ªèƒ½é€’å½’å–åˆ° Tensor çš„å…ƒç´ 
    - æœ‰ 'last_hidden_state' å±æ€§: å–ä¹‹
    - dict: ä¼˜å…ˆå°è¯•è¿™äº›é”®: 'last_hidden_state', 'hidden_states', 'out', 'output', 0
    å–ä¸åˆ°å°±è¿”å› Noneã€‚
    """
    if isinstance(obj, torch.Tensor):
        return obj
    # ModelOutput (transformers) é€šå¸¸å¯å½“ dict å¤„ç†
    if isinstance(obj, (list, tuple)):
        for it in obj:
            t = extract_tensor(it)
            if t is not None:
                return t
        return None
    if isinstance(obj, dict):
        for k in ("last_hidden_state", "hidden_states", "out", "output", 0, "x"):
            if k in obj:
                t = extract_tensor(obj[k])
                if t is not None:
                    return t
        # fallback: ä»»æ„å€¼é‡Œæ‰¾ä¸€ä¸ª tensor
        for v in obj.values():
            t = extract_tensor(v)
            if t is not None:
                return t
        return None
    # æœ‰å±æ€§çš„å¯¹è±¡ï¼ˆå¦‚ ModelOutputï¼‰
    for attr in ("last_hidden_state", "hidden_states", "out", "output"):
        if hasattr(obj, attr):
            t = extract_tensor(getattr(obj, attr))
            if t is not None:
                return t
    return None

class BlockIOCollector:
    """
    ç»™æ¯ä¸ª block åŒæ—¶æŒ‚ pre-hook / forward-hookï¼Œ
    æ”¶é›†æ¯æ¬¡ forward çš„ (x_in, x_out) å¯¹ã€‚
    ä¿®å¤ç‚¹ï¼šå¯¹ tuple/ModelOutput è¿›è¡Œç¨³å¥çš„å¼ é‡æå–ã€‚
    """
    def __init__(self, blocks: nn.ModuleList):
        self.blocks = blocks
        self.io_pairs: List[Tuple[torch.Tensor, torch.Tensor]] = []
        self._handles = []
        self._caches: List[Dict[str, Optional[torch.Tensor]]] = []

    def _register(self):
        self._handles.clear()
        self._caches = [{"in": None, "out": None} for _ in self.blocks]

        for i, blk in enumerate(self.blocks):
            def pre_hook(module, inputs, idx=i):
                # å¯¹ inputs è¿›è¡Œç¨³å¥æå–ï¼›é€šå¸¸ inputs[0] å°±æ˜¯ hidden_states
                x_in = None
                if isinstance(inputs, tuple) and len(inputs) > 0:
                    x_in = extract_tensor(inputs[0])
                if x_in is None:
                    x_in = extract_tensor(inputs)
                if x_in is not None:
                    self._caches[idx]["in"] = x_in.detach()

            def fwd_hook(module, inputs, output, idx=i):
                # output å¯èƒ½æ˜¯ tuple/ModelOutputï¼Œéœ€è¦æå–ç¬¬ä¸€ä¸»è¾“å‡º
                x_out = extract_tensor(output)
                if x_out is not None:
                    self._caches[idx]["out"] = x_out.detach()

            self._handles.append(blk.register_forward_pre_hook(pre_hook))
            self._handles.append(blk.register_forward_hook(fwd_hook))

    def __enter__(self):
        self._register()
        return self

    def __exit__(self, exc_type, exc, tb):
        for h in self._handles:
            h.remove()
        self._handles.clear()

    def collect_once(self):
        """
        åœ¨ä¸€æ¬¡ forward ç»“æŸåï¼ŒæŠŠç¼“å­˜ä¸­çš„ (in, out) ä¾æ¬¡å–å‡ºï¼Œå½¢æˆåˆ—è¡¨ã€‚
        ç„¶åæ¸…ç©ºç¼“å­˜ï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡ forwardã€‚
        """
        self.io_pairs = []
        for cache in self._caches:
            x_in, x_out = cache["in"], cache["out"]
            self.io_pairs.append((x_in, x_out))
            cache["in"] = None
            cache["out"] = None
        return self.io_pairs

# ======================
# å¯»æ‰¾ encoder blocks
# ======================
def find_encoder_blocks(model: nn.Module) -> Tuple[nn.Module, str, nn.ModuleList]:
    """
    è‡ªåŠ¨å®šä½ SegGPT çš„ç¼–ç å™¨ blocksï¼ˆModuleListï¼‰ã€‚
    å¸¸è§è·¯å¾„ï¼šmodel.seggpt.encoder.blocks / model.model.encoder.blocks / ... / .layers / .layer
    """
    candidates = [getattr(model, "seggpt", None), getattr(model, "model", None), model]
    for root in candidates:
        if root is None:
            continue
        enc = getattr(root, "encoder", root)
        for name in ["blocks", "layers", "layer"]:
            if hasattr(enc, name):
                blocks = getattr(enc, name)
                if isinstance(blocks, nn.ModuleList) and len(blocks) > 0:
                    return enc, name, blocks
    raise RuntimeError("Cannot locate encoder blocks ModuleList. Use print(model) to inspect structure.")
# ======================
# è£å‰ªå‡½æ•°
# ======================
def prune_model(model, blocks, sims_mean, prune_ratio=0.3, keep_first=2, keep_last=2):
    num_blocks = len(blocks)
    k_prune = int(num_blocks * prune_ratio)
    indices = list(range(num_blocks))
    # ç›¸ä¼¼åº¦é«˜ -> æ›´å†—ä½™
    sorted_by_sim = sorted(indices, key=lambda i: (sims_mean[i] if sims_mean[i] is not None else -1), reverse=True)
    protected = set(list(range(keep_first)) + list(range(num_blocks - keep_last, num_blocks)))
    prune_candidates = [i for i in sorted_by_sim if i not in protected]
    prune_indices = set(prune_candidates[:k_prune])
    keep_indices  = [i for i in indices if i not in prune_indices]
    pruned_model = copy.deepcopy(model)
    # é‡å»º ModuleList
    encoder = getattr(pruned_model, "seggpt", None) or getattr(pruned_model, "model", None) or pruned_model
    encoder = getattr(encoder, "encoder", encoder)
    blocks_attr = None
    for name in ["layers", "blocks", "layer"]:
        if hasattr(encoder, name):
            blocks_attr = name
            old_blocks = getattr(encoder, name)
            break
    assert blocks_attr is not None and isinstance(old_blocks, nn.ModuleList)
    kept_blocks = [old_blocks[i] for i in keep_indices]
    setattr(encoder, blocks_attr, nn.ModuleList(kept_blocks))
    # æ›´æ–°å±‚æ•°
    if hasattr(pruned_model.config, "num_hidden_layers"):
        pruned_model.config.num_hidden_layers = len(keep_indices)
    # ğŸ”´ å…³é”®ï¼šé‡æ˜ å°„ä¸­é—´å±‚ç´¢å¼•ï¼Œä¿æŒä¸ªæ•°ä¸å˜
    if hasattr(pruned_model.config, "intermediate_hidden_state_indices") and pruned_model.config.intermediate_hidden_state_indices:
        orig_inter = list(pruned_model.config.intermediate_hidden_state_indices)
        new_inter  = remap_intermediate_indices_after_prune(keep_indices, orig_inter)
        pruned_model.config.intermediate_hidden_state_indices = new_inter
        print("[Prune] remapped intermediate indices:", orig_inter, "->", new_inter)
    pruned_model.eval()
    return pruned_model, keep_indices, prune_indices

def remap_intermediate_indices_after_prune(
        keep_indices,                   # List[int]  å‰ªåä¿ç•™çš„â€œåŸå§‹å±‚å·â€ï¼Œå‡åº
        orig_inter_indices,             # List[int]  å‰ªå‰ config.intermediate_hidden_state_indices
) -> list:
    """
    æŠŠâ€œåŸå§‹å±‚å·çš„ä¸­é—´å±‚ç´¢å¼•â€æ˜ å°„åˆ°â€œå‰ªåæ¨¡å‹ä¸­çš„å±‚å·â€(å³ä¿ç•™ä¸‹æ¥å±‚åœ¨æ–°ModuleListä¸­çš„ä½ç½®)ã€‚
    è‹¥æŸä¸ªåŸç´¢å¼•è¢«åˆ é™¤ï¼Œåˆ™é€‰æ‹©è·ç¦»æœ€è¿‘çš„ä¿ç•™å±‚æ›¿ä»£ã€‚
    è¿”å›ï¼šå‰ªåæ¨¡å‹çš„ indicesï¼ˆä»ä¿æŒä¸åŸæ¥ç›¸åŒçš„ä¸ªæ•°ï¼‰ã€‚
    """
    # åŸå§‹å±‚å· -> æ–°ä½ç½®ï¼ˆ0..len(keep)-1ï¼‰
    pos_in_keep = {orig_idx: new_i for new_i, orig_idx in enumerate(keep_indices)}
    def nearest_kept(orig_idx):
        # æ‰¾ä¸ orig_idx è·ç¦»æœ€è¿‘çš„ä¸€ä¸ª keep_indices å…ƒç´ 
        return min(keep_indices, key=lambda k: abs(k - orig_idx))
    new_indices = []
    for oi in orig_inter_indices:
        if oi in pos_in_keep:
            new_indices.append(pos_in_keep[oi])
        else:
            nearest_orig = nearest_kept(oi)
            new_indices.append(pos_in_keep[nearest_orig])
    # å¯èƒ½ä¼šå‡ºç°é‡å¤ï¼ˆå¤šä¸ªåŸç´¢å¼•æ˜ åˆ°åŒä¸€ä¸ªæ–°å±‚ï¼‰ï¼›decoder ä¸€èˆ¬åªè¦æ±‚ä¸ªæ•°å’Œç»´åº¦ä¸€è‡´ï¼Œ
    # å…è®¸é‡å¤å¹¶ä¸å½±å“å°ºå¯¸ã€‚å¦‚æœä½ æƒ³å°½é‡å»é‡ï¼Œå¯åœ¨æ­¤åšè½»å¾®å¾®è°ƒï¼Œä½†åŠ¡å¿…ä¿æŒä¸ªæ•°ä¸å˜ã€‚
    return new_indices

# ======================
# ä¸»æµç¨‹
# ======================
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    log(f"Using device: {DEVICE}")

    # è½½å…¥å¤„ç†å™¨ä¸æ¨¡å‹
    image_processor = SegGptImageProcessor.from_pretrained(CHECKPOINT, local_files_only=True)
    model = SegGptForImageSegmentation.from_pretrained(CHECKPOINT, local_files_only=True)
    model.to(DEVICE).eval()

    # å‡†å¤‡æç¤ºå›¾ä¸æ©ç 
    prompt_image_path = os.path.join(PROMPT_DIR, "save4_2.jpeg")
    prompt_mask_path  = os.path.join(PROMPT_DIR, "save4_2_mask.jpeg")
    if not (os.path.exists(prompt_image_path) and os.path.exists(prompt_mask_path)):
        raise FileNotFoundError("Prompt image/mask not found in PROMPT_DIR.")

    prompt_images = [Image.open(prompt_image_path)]
    prompt_masks  = [Image.open(prompt_mask_path).convert("L")]

    # æ‰¾åˆ° blocks
    encoder, blocks_name, blocks = find_encoder_blocks(model)
    num_blocks = len(blocks)
    log(f"Found encoder ModuleList '{blocks_name}' with {num_blocks} blocks.")

    # ç»Ÿè®¡å®¹å™¨
    layer_sims_sum = torch.zeros(num_blocks, dtype=torch.float64, device=DEVICE)
    layer_counts   = torch.zeros(num_blocks, dtype=torch.int32, device=DEVICE)

    # å¾…ç»Ÿè®¡å›¾ç‰‡
    all_imgs = list_images(INPUT_DIR)
    if len(all_imgs) == 0:
        raise FileNotFoundError(f"No images in {INPUT_DIR}")
    if MAX_CALIB > 0:
        all_imgs = all_imgs[:MAX_CALIB]

    log(f"Calibrating with {len(all_imgs)} images ...")
    t0 = time.time()

    with torch.no_grad():
        with BlockIOCollector(blocks) as collector:
            for idx, img_path in enumerate(all_imgs, 1):
                img = Image.open(img_path)
                inputs = image_processor(
                    images=img,
                    prompt_images=prompt_images,
                    prompt_masks=prompt_masks,
                    return_tensors="pt",
                    feature_ensemble=FEATURE_ENSEMBLE
                )
                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

                # å‰å‘ï¼šåªä¸ºè§¦å‘ hooksï¼Œä¸éœ€è¦å– outputs
                _ = model(**inputs)

                io_pairs = collector.collect_once()
                if len(io_pairs) != num_blocks:
                    log(f"Warning: collected {len(io_pairs)} IO pairs, expect {num_blocks}")

                per_image_sims = []
                for l, (x_in, x_out) in enumerate(io_pairs):
                    if (x_in is None) or (x_out is None):
                        per_image_sims.append(None)
                        continue
                    try:
                        sim = cosine_mean(x_in, x_out)
                        layer_sims_sum[l] += sim.to(layer_sims_sum.dtype)
                        layer_counts[l]   += 1
                        per_image_sims.append(float(sim.detach().cpu()))
                    except Exception as e:
                        log(f"[img#{idx}] Skip layer {l} due to: {repr(e)}")
                        per_image_sims.append(None)

                log(f"[{idx:>3}/{len(all_imgs)}] {os.path.basename(img_path)} "
                    f"per-block sims (sample): {per_image_sims[:min(6, num_blocks)]}")

    # æ±‡æ€»
    sims_mean = []
    for l in range(num_blocks):
        if layer_counts[l].item() > 0:
            sims_mean.append(float((layer_sims_sum[l] / layer_counts[l]).detach().cpu()))
        else:
            sims_mean.append(None)

    # è¾“å‡ºä¸ä¿å­˜
    log("=== Per-block mean cosine similarity ===")
    for i, v in enumerate(sims_mean):
        log(f"Block {i:02d}: {v if v is not None else 'N/A'}")

    stats = {
        "checkpoint": CHECKPOINT,
        "num_blocks": num_blocks,
        "feature_ensemble": FEATURE_ENSEMBLE,
        "images_used": len(all_imgs),
        "cosine_mean_per_block": sims_mean,
    }
    out_json = os.path.join(OUTPUT_DIR, "block_cosine_stats.json")
    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    t1 = time.time()
    log(f"Saved stats to {out_json}")
    log(f"Total time: {t1 - t0:.2f}s")

    # è£å‰ª 30% çš„ block
    pruned_model, keep_indices, prune_indices = prune_model(
        model, blocks, sims_mean, prune_ratio=0.4, keep_first=2, keep_last=2
    )

    pruned_dir = "result/test_dir/pruned_seggpt_40"
    os.makedirs(pruned_dir, exist_ok=True)
    pruned_model.save_pretrained(pruned_dir)
    image_processor.save_pretrained(pruned_dir)  # æ–¹ä¾¿ç¦»çº¿ä¸€å¹¶åŠ è½½
    with open(os.path.join(pruned_dir,"keep_indices.json"),"w") as f:
        json.dump({"keep_indices": keep_indices}, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()